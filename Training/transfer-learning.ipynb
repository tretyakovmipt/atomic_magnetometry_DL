{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Here we use a model pretrained on the vanilla dataset and fine-tune it on a cavity dataset with and without noise, and compare its performance with a model trained on these datasets from scratch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import external libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load cavity datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = np.load('..\\data\\X_train_c.npy')  # population in F=1\n",
    "Y_train = np.load('..\\data\\Y_train_c.npy')  #normalized to w_L\n",
    "\n",
    "X_test = np.load('..\\data\\X_val_C.npy')  # use validation set for testing\n",
    "Y_test = np.load('..\\data\\Y_val_C.npy')  # use validation set for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalize X data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Level = np.min(X_train)\n",
    "Height = np.max(X_train) - np.min(X_train)\n",
    "\n",
    "X_train = (X_train - Level) / Height\n",
    "X_test = (X_test - Level) / Height"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shuffle training and validation sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m_train = np.random.permutation(len(X_train)) # Shuffling mask for X_train\n",
    "m_test = np.random.permutation(len(X_test)) # Shuffling mask for X_test\n",
    "\n",
    "X_train, Y_train = X_train[m_train], Y_train[m_train]\n",
    "X_test, Y_test = X_test[m_test], Y_test[m_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reshape X for CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = X_train[:, :, np.newaxis]\n",
    "X_test = X_test[:, :, np.newaxis]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Introduce training callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=500, monitor='val_loss', restore_best_weights=True) # Stop if the validation loss is not improving\n",
    "learning_rate_cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100) # Decrease the learning rate if the validation loss is not improving\n",
    "cb_list = [early_stopping_cb, learning_rate_cb]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training on cavity data without noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_cnn = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(kernel_size=16, activation='relu', padding='SAME', filters=32),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Conv1D(kernel_size=8, activation='relu', padding='SAME', filters=64),\n",
    "    keras.layers.Conv1D(kernel_size=8, activation='relu', padding='SAME', filters=64),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Conv1D(kernel_size=4, activation='relu', padding='SAME', filters=128),\n",
    "    keras.layers.Conv1D(kernel_size=4, activation='relu', padding='SAME', filters=128),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[\"mae\"])  #loss functions are given for two output neurons"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pretrained_model = keras.models.load_model('Training_data\\model_cnn1.h5')\n",
    "pretrained_model.compile(loss='mae', optimizer=keras.optimizers.Adam(),\n",
    "                         metrics=['mae'])  #loss functions are given for two output neurons\n",
    "\n",
    "history_pretrained = pretrained_model.fit(X_train, Y_train, epochs=2000, validation_data=(X_test, Y_test),\n",
    "                                          batch_size=32, callbacks=[checkpoint_cb, early_stopping_cb, learning_rate_cb])\n",
    "\n",
    "history_cnn = model_cnn.fit(X_train, Y_train, epochs=2000, validation_data=(X_test, Y_test), batch_size=32,\n",
    "                            callbacks=[checkpoint_cb, early_stopping_cb, learning_rate_cb])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h_cnn = history_cnn.history\n",
    "\n",
    "cnn_training_data = pd.DataFrame({\"CNN training loss\": h_cnn[\"loss\"], \"CNN test_loss\": h_cnn[\"val_loss\"]})\n",
    "cnn_training_data.index.name = 'CNN Epoch'\n",
    "\n",
    "fig_cnn = cnn_training_data.plot()\n",
    "fig_cnn.set_xlabel(r'epoch_count', fontsize=12)\n",
    "fig_cnn.set_ylabel(r'Loss', fontsize=12)\n",
    "fig_cnn.set_ylim(0, 0.05)\n",
    "\n",
    "y = model_cnn.predict(X_test)\n",
    "print('max error:', np.max(np.abs(y - Y_test)))\n",
    "print('mean absolute error:', np.mean(np.sqrt((y - Y_test) ** 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h_pretrained = history_pretrained.history\n",
    "\n",
    "pretrained_training_data = pd.DataFrame(\n",
    "    {\"Transfer training loss\": h_pretrained[\"loss\"], \"Transfer test_loss\": h_pretrained[\"val_loss\"]})\n",
    "pretrained_training_data.index.name = 'Transfer Epoch'\n",
    "\n",
    "fig_pretrained = pretrained_training_data.plot()\n",
    "fig_pretrained.set_xlabel(r'epoch_count', fontsize=12)\n",
    "fig_pretrained.set_ylabel(r'Loss', fontsize=12)\n",
    "fig_pretrained.set_ylim(0, 0.05)\n",
    "\n",
    "y = pretrained_model.predict(X_test)\n",
    "print('max error:', np.max(np.abs(y - Y_test)))\n",
    "print('mean absolute error:', np.mean(np.sqrt((y - Y_test) ** 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training with noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "noise_level = 0.01\n",
    "\n",
    "noise = np.random.normal(0, noise_level, X_train.shape)\n",
    "X_train_N = X_train + noise\n",
    "\n",
    "noise = np.random.normal(0, noise_level, X_test.shape)\n",
    "X_test_N = X_test + noise\n",
    "\n",
    "model_cnn = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(kernel_size=16, activation='relu', padding='SAME', filters=32),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Conv1D(kernel_size=8, activation='relu', padding='SAME', filters=64),\n",
    "    keras.layers.Conv1D(kernel_size=8, activation='relu', padding='SAME', filters=64),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Conv1D(kernel_size=4, activation='relu', padding='SAME', filters=128),\n",
    "    keras.layers.Conv1D(kernel_size=4, activation='relu', padding='SAME', filters=128),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[\"mae\"])  #loss functions are given for two output neurons\n",
    "\n",
    "history_cnn = model_cnn.fit(X_train_N, Y_train, epochs=2000, validation_data=(X_test_N, Y_test), batch_size=32,\n",
    "                            callbacks=[checkpoint_cb, early_stopping_cb, learning_rate_cb])\n",
    "\n",
    "h_cnn = history_cnn.history\n",
    "\n",
    "cnn_training_data = pd.DataFrame({\"CNN training loss\": h_cnn[\"loss\"], \"CNN test_loss\": h_cnn[\"val_loss\"]})\n",
    "cnn_training_data.index.name = 'CNN Epoch'\n",
    "\n",
    "fig_cnn = cnn_training_data.plot()\n",
    "fig_cnn.set_xlabel(r'epoch_count', fontsize=12)\n",
    "fig_cnn.set_ylabel(r'Loss', fontsize=12)\n",
    "fig_cnn.set_ylim(0, 0.05)\n",
    "\n",
    "y = model_cnn.predict(X_test_N)\n",
    "print('max error:', np.max(np.abs(y - Y_test)))\n",
    "print('mean absolute error:', np.mean(np.sqrt((y - Y_test) ** 2)))\n",
    "\n",
    "# #save model\n",
    "# model_cnn.save('Training_data\\with_noise\\one_percent\\model_cavity_scratch.h5')\n",
    "\n",
    "# #save data\n",
    "# cnn_training_data.to_csv('Training_data\\with_noise\\one_percent\\data_cavity_scratch.csv')\n",
    "\n",
    "pretrained_model_N = keras.models.load_model('Training_data\\model_cnn1.h5')\n",
    "pretrained_model_N.compile(loss='mae', optimizer=keras.optimizers.Adam(),\n",
    "                           metrics=['mae'])  #loss functions are given for two output neurons\n",
    "\n",
    "history_pretrained = pretrained_model_N.fit(X_train_N, Y_train, epochs=2000, validation_data=(X_test_N, Y_test),\n",
    "                                            batch_size=32,\n",
    "                                            callbacks=[checkpoint_cb, early_stopping_cb, learning_rate_cb])\n",
    "\n",
    "h_pretrained = history_pretrained.history\n",
    "\n",
    "pretrained_training_data = pd.DataFrame(\n",
    "    {\"Transfer training loss\": h_pretrained[\"loss\"], \"Transfer test_loss\": h_pretrained[\"val_loss\"]})\n",
    "pretrained_training_data.index.name = 'Transfer Epoch'\n",
    "\n",
    "fig_pretrained = pretrained_training_data.plot()\n",
    "fig_pretrained.set_xlabel(r'epoch_count', fontsize=12)\n",
    "fig_pretrained.set_ylabel(r'Loss', fontsize=12)\n",
    "fig_pretrained.set_ylim(0, 0.05)\n",
    "\n",
    "y = pretrained_model.predict(X_test)\n",
    "print('max error:', np.max(np.abs(y - Y_test)))\n",
    "print('mean absolute error:', np.mean(np.sqrt((y - Y_test) ** 2)))\n",
    "\n",
    "# #save model\n",
    "# model_cnn.save('Training_data\\with_noise\\one_percent\\model_cavity_finetuned.h5')\n",
    "\n",
    "# #save data\n",
    "# cnn_training_data.to_csv('Training_data\\with_noise\\one_percent\\data_cavity_finetuned.csv')\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "filter_model = load_model('Training_data\\with_noise\\one_percent\\model_denoise.h5')\n",
    "tuned_cnn_model = load_model('Training_data\\model_cavity_transfer.h5')\n",
    "y = tuned_cnn_model.predict(X_test_N)\n",
    "err = np.abs(Y_test - y)\n",
    "err_df = pd.DataFrame({'W_par': err[:, 0], 'W_ort': err[:, 1]})\n",
    "err_df.plot()\n",
    "\n",
    "y_filtered = tuned_cnn_model.predict(filter_model.predict(X_test_N))\n",
    "err_filtered = np.abs(Y_test - y_filtered)\n",
    "err_filtered_df = pd.DataFrame({'W_par_filtered': err_filtered[:, 0], 'W_ort_filtered': err_filtered[:, 1]})\n",
    "err_filtered_df.plot()\n",
    "\n",
    "X_filtered = filter_model.predict(X_test_N)\n",
    "\n",
    "index = 150\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(320), X_filtered[index, :, 0])\n",
    "ax.plot(range(320), X_test_N[index, :, 0])\n",
    "ax.legend(loc=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}